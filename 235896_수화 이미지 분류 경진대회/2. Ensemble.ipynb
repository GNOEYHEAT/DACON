{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble - CBAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112690,
     "status": "ok",
     "timestamp": 1645972270802,
     "user": {
      "displayName": "‍김태형[ 대학원석·박사통합과정재학 / 산업경영공학과 ]",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFNpnkjN-Em0rOki5hhy0HR7yGAbxSpzCjHV0A=s64",
      "userId": "00288066936238655028"
     },
     "user_tz": -540
    },
    "id": "A1IbqGhzB7fy",
    "outputId": "fd656f36-afcb-4871-ee1c-557fd033af76"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "from imgaug import augmenters as iaa\n",
    "import imgaug as ia\n",
    "\n",
    "import argparse\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"DACON_235896\", name=\"Ensemble\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Ensemble')\n",
    "parser.add_argument('--resize_size', default=224, type=int)\n",
    "parser.add_argument('--randaugment_n', default=3, type=int)\n",
    "parser.add_argument('--randaugment_m', default=7, type=int)\n",
    "parser.add_argument('--optimizer', default=\"sgd\", type=str) # sgd or adam\n",
    "parser.add_argument('--learning_rate', default=0.01, type=float)\n",
    "parser.add_argument('--batch_size', default=128, type=int)\n",
    "parser.add_argument('--epochs', default=200, type=int)\n",
    "parser.add_argument('--n_splits', default=10, type=int)\n",
    "parser.add_argument('--seed', default=1011, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "wandb.config.update(args)\n",
    "\n",
    "resize_size=args.resize_size\n",
    "BATCH_SIZE=args.batch_size\n",
    "EPOCHS=args.epochs\n",
    "N_SPLITS=args.n_splits\n",
    "SEED=args.seed\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    ia.seed(seed)\n",
    "\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_load(path):\n",
    "    img = cv2.imread(path)[:,:,::-1]\n",
    "    img = cv2.resize(img, (resize_size, resize_size))\n",
    "    return img\n",
    "\n",
    "X = np.array([img_load(i) for i in tqdm(glob('data/train/*.png'))])\n",
    "X_test = np.array([img_load(i) for i in tqdm(glob('data/test/*.png'))])\n",
    "\n",
    "y = pd.read_csv(\"data/train.csv\")[\"label\"]\n",
    "y_encoder = {key : value for key, value in zip(np.unique(y), range(len(np.unique(y))))}\n",
    "y = np.array([y_encoder[k] for k in y])\n",
    "\n",
    "X.shape, X_test.shape, y.shape, X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"data/train.csv\")[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in y_encoder.items():\n",
    "    plt.figure(figsize=(21,20))\n",
    "    for i in range(10):\n",
    "        plt.subplot(1, 11, i+1)\n",
    "        plt.imshow(X[y==v][i])\n",
    "        plt.title(k)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(input_feature, ratio=8):\n",
    "    \n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature.shape[channel_axis]\n",
    "    \n",
    "    se_feature = layers.GlobalAveragePooling2D()(input_feature)\n",
    "    se_feature = layers.Reshape((1, 1, channel))(se_feature)\n",
    "    se_feature = layers.Dense(channel // ratio,\n",
    "                              activation='relu',\n",
    "                              kernel_initializer='he_normal',\n",
    "                              use_bias=True,\n",
    "                              bias_initializer='zeros')(se_feature)\n",
    "    se_feature = layers.Dense(channel,\n",
    "                              activation='sigmoid',\n",
    "                              kernel_initializer='he_normal',\n",
    "                              use_bias=True,\n",
    "                              bias_initializer='zeros')(se_feature)\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        se_feature = layers.Permute((3, 1, 2))(se_feature)\n",
    "        \n",
    "    se_feature = layers.multiply([input_feature, se_feature])\n",
    "    \n",
    "    return se_feature\n",
    "\n",
    "\n",
    "def cbam_block(cbam_feature, ratio=8):\n",
    "    \n",
    "    cbam_feature = channel_attention(cbam_feature, ratio)\n",
    "    cbam_feature = spatial_attention(cbam_feature)\n",
    "    \n",
    "    return cbam_feature\n",
    "\n",
    "\n",
    "def channel_attention(input_feature, ratio=8):\n",
    "    \n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature.shape[channel_axis]\n",
    "    \n",
    "    shared_layer_one = layers.Dense(channel//ratio,\n",
    "                                    activation='relu',\n",
    "                                    kernel_initializer='he_normal',\n",
    "                                    use_bias=True,\n",
    "                                    bias_initializer='zeros')\n",
    "    shared_layer_two = layers.Dense(channel,\n",
    "                                    kernel_initializer='he_normal',\n",
    "                                    use_bias=True,\n",
    "                                    bias_initializer='zeros')\n",
    "\n",
    "    avg_pool = layers.GlobalAveragePooling2D()(input_feature)\n",
    "    avg_pool = layers.Reshape((1,1,channel))(avg_pool)\n",
    "    avg_pool = shared_layer_one(avg_pool)\n",
    "    avg_pool = shared_layer_two(avg_pool)\n",
    "    \n",
    "    max_pool = layers.GlobalMaxPooling2D()(input_feature)\n",
    "    max_pool = layers.Reshape((1,1,channel))(max_pool)\n",
    "    max_pool = shared_layer_one(max_pool)\n",
    "    max_pool = shared_layer_two(max_pool)\n",
    "    \n",
    "    cbam_feature = layers.Add()([avg_pool,max_pool])\n",
    "    cbam_feature = layers.Activation('sigmoid')(cbam_feature)\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = layers.Permute((3, 1, 2))(cbam_feature)\n",
    "        \n",
    "    return layers.multiply([input_feature, cbam_feature])\n",
    "\n",
    "\n",
    "def spatial_attention(input_feature):\n",
    "    \n",
    "    kernel_size = 7\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        channel = input_feature.shape[1]\n",
    "        cbam_feature = layers.Permute((2,3,1))(input_feature)\n",
    "    else:\n",
    "        channel = input_feature.shape[-1]\n",
    "        cbam_feature = input_feature\n",
    "        \n",
    "    avg_pool = layers.Lambda(lambda x: K.mean(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    max_pool = layers.Lambda(lambda x: K.max(x, axis=3, keepdims=True))(cbam_feature)\n",
    "    concat = layers.Concatenate(axis=3)([avg_pool, max_pool])\n",
    "    cbam_feature = layers.Conv2D(filters = 1,\n",
    "                                 kernel_size=kernel_size,\n",
    "                                 strides=1,\n",
    "                                 padding='same',\n",
    "                                 activation='sigmoid',\n",
    "                                 kernel_initializer='he_normal',\n",
    "                                 use_bias=False)(concat)\n",
    "    \n",
    "    if K.image_data_format() == \"channels_first\":\n",
    "        cbam_feature = layers.Permute((3, 1, 2))(cbam_feature)\n",
    "        \n",
    "    return layers.multiply([input_feature, cbam_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cbam(activation='relu'):\n",
    "\n",
    "    inp = tf.keras.Input(shape=(resize_size, resize_size, 3))\n",
    "    x = layers.Rescaling(1./255)(inp)\n",
    "\n",
    "    x = layers.Conv2D(32, 3, 1, \"same\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv2D(32, 3, 1, \"same\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = cbam_block(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, 1, \"same\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv2D(64, 3, 1, \"same\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = cbam_block(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(128, 3, 1, \"same\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv2D(128, 3, 1, \"same\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = cbam_block(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(256, 3, 1, \"same\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv2D(256, 3, 1, \"same\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = cbam_block(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "\n",
    "    x = layers.Conv2D(512, 3, 1, \"same\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv2D(512, 3, 1, \"same\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = cbam_block(x)\n",
    "\n",
    "    x = layers.Conv2D(1024, 3, 1, \"same\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = layers.Conv2D(1024, 3, 1, \"same\", kernel_initializer='he_normal')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(activation)(x)\n",
    "    x = cbam_block(x)\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "    x = layers.Dense(256, activation=activation)(x)\n",
    "    x = layers.Dense(64, activation=activation)(x)\n",
    "    oup = layers.Dense(len(np.unique(y)), activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=inp, outputs=oup)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_aug = iaa.RandAugment(n=args.randaugment_n, m=args.randaugment_m)\n",
    "\n",
    "def augment(images):\n",
    "    images = tf.cast(images, tf.uint8)\n",
    "    return rand_aug(images=images.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tta(model, imgs):\n",
    "    tta_pred=[]\n",
    "    tta_pred.append(model.predict(imgs))\n",
    "    tta_pred.append(model.predict(imgs[:,:,::-1]))\n",
    "    return np.mean(tta_pred, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0\n",
    "pred_ensemble=[]\n",
    "\n",
    "skf = StratifiedKFold(n_splits=N_SPLITS)\n",
    "for train_index, val_index in skf.split(X, y):\n",
    "    \n",
    "    idx+=1\n",
    "    \n",
    "    X_train, X_val = X[train_index], X[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    train_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "        .shuffle(len(X_train))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .map(lambda x, y: (tf.py_function(augment, [x], [tf.float32])[0], y),\n",
    "             num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    val_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    model = cbam()\n",
    "    \n",
    "    lr = tf.keras.optimizers.schedules.CosineDecay(args.learning_rate, decay_steps=1000)\n",
    "    if args.optimizer == \"sgd\":\n",
    "        optim = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "    elif args.optimizer == \"adam\":\n",
    "        optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer=optim,\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath=f\"load_model/{parser.description}_{idx}\"\n",
    "\n",
    "    checkpoint_callback = [\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoint_filepath,\n",
    "            monitor=\"val_loss\",\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint_callback, WandbCallback()],\n",
    "    )\n",
    "\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "\n",
    "    loss=history.history['loss']\n",
    "    val_loss=history.history['val_loss']\n",
    "\n",
    "    plt.plot(acc, label='Training Accuracy')\n",
    "    plt.plot(val_acc, label='Validation Accuracy')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(loss, label='Training Loss')\n",
    "    plt.plot(val_loss, label='Validation Loss')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.show()\n",
    "    \n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    \n",
    "    pred_prob = tta(model, X_test)\n",
    "    pred_ensemble.append(pred_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = np.mean(pred_ensemble, axis=0)\n",
    "pred = np.argmax(pred_prob, axis=1)\n",
    "y_decoder = {value : key for key, value in y_encoder.items()}\n",
    "pred = np.array([y_decoder[v] for v in pred])\n",
    "\n",
    "pd.Series(pred).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=pd.Series(pred).value_counts().min()\n",
    "\n",
    "for k, v in y_decoder.items():\n",
    "    plt.figure(figsize=(21,20))\n",
    "    for i in range(n):\n",
    "        plt.subplot(1, n+1, i+1)\n",
    "        plt.imshow(X_test[pred==v][i])\n",
    "        plt.title(v)\n",
    "        plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "submission[\"label\"] = pred\n",
    "submission.to_csv(f\"{parser.description}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPxWDbXdBZDPv2XiwW8C1k0",
   "collapsed_sections": [],
   "mount_file_id": "13o4BpF8zzuXcEiNVlVG2KwTXMW1Y55_v",
   "name": "test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
