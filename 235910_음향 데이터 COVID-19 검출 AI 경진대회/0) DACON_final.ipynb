{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uO10XRn3dn1L"
   },
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1655180838758,
     "user": {
      "displayName": "‍김태형[ 대학원석·박사통합과정재학 / 산업경영공학과 ]",
      "userId": "00288066936238655028"
     },
     "user_tz": -540
    },
    "id": "qlG--rMUIhtl",
    "outputId": "ae8c4164-a38a-469d-9365-ca22d43833e9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "import librosa\n",
    "from audiomentations import Compose, AddGaussianNoise, TimeStretch, PitchShift, Shift\n",
    "\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pickle\n",
    "import argparse\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"COVID-19\", name=\"Baseline\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description=\"Baseline\")\n",
    "parser.add_argument('--sampling_rate', default=16000, type=int)\n",
    "parser.add_argument('--top_db', default=60, type=int)\n",
    "parser.add_argument('--feature', default=\"melspec\", type=str) # melspec or mfcc \n",
    "parser.add_argument('--wav_aug', default=True, type=bool)\n",
    "parser.add_argument('--spec_aug', default=True, type=bool)\n",
    "parser.add_argument('--freq_mask', default=32, type=int) # 8, 16, 32\n",
    "parser.add_argument('--time_mask', default=32, type=int) # 8, 16, 32\n",
    "parser.add_argument('--n_freq_mask', default=1, type=int) # 1, 2\n",
    "parser.add_argument('--n_time_mask', default=1, type=int) # 1, 2\n",
    "parser.add_argument('--optimizer', default=\"sgd\", type=str) # sgd or adam\n",
    "parser.add_argument('--loss', default=\"fl\", type=str) # bc or fl or fl_af\n",
    "parser.add_argument('--learning_rate', default=0.001, type=float)\n",
    "parser.add_argument('--batch_size', default=64, type=int)\n",
    "parser.add_argument('--epochs', default=100, type=int)\n",
    "parser.add_argument('--cv', default=10, type=int)\n",
    "parser.add_argument('--ensemble', default=\"soft\", type=str) # soft or hard\n",
    "parser.add_argument('--seed', default=1011, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "wandb.config.update(args)\n",
    "\n",
    "sampling_rate = args.sampling_rate\n",
    "top_db = args.top_db\n",
    "feature = args.feature\n",
    "wav_aug = args.wav_aug\n",
    "spec_aug = args.spec_aug\n",
    "freq_mask = args.freq_mask\n",
    "time_mask = args.time_mask\n",
    "n_freq_mask = args.n_freq_mask\n",
    "n_time_mask = args.n_time_mask\n",
    "optimizer = args.optimizer\n",
    "loss = args.loss\n",
    "learning_rate = args.learning_rate\n",
    "BATCH_SIZE = args.batch_size\n",
    "EPOCHS = args.epochs\n",
    "cv = args.cv\n",
    "ensemble = args.ensemble\n",
    "seed = args.seed\n",
    "\n",
    "def set_seeds(seed=seed):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "set_seeds()\n",
    "\n",
    "train_df = pd.read_csv(\"data/train_data.csv\")\n",
    "test_df = pd.read_csv(\"data/test_data.csv\")\n",
    "\n",
    "train_folder = \"data/train/\"\n",
    "test_folder = \"data/test/\"\n",
    "\n",
    "def dataset(folder, df):\n",
    "    dataset = []\n",
    "    for uid in tqdm(df['id']):\n",
    "        path = os.path.join(folder, str(uid).zfill(5)+'.wav')\n",
    "        y, sr = librosa.load(path, sr=sampling_rate)\n",
    "        y = librosa.util.normalize(y)\n",
    "        dataset.append([y])\n",
    "    dataset = pd.DataFrame(dataset, columns=['data'])\n",
    "    dataset = pd.concat([dataset, df], axis=1)\n",
    "    return dataset\n",
    "\n",
    "train_df = dataset(train_folder, train_df)\n",
    "test_df = dataset(test_folder, test_df)\n",
    "\n",
    "train_df[\"covid19\"].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SLr-znikdl4b"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_wav(data, top_db):\n",
    "    \n",
    "    frame_length = 0.025\n",
    "    frame_stride = 0.010\n",
    "\n",
    "    input_nfft = int(round(sampling_rate*frame_length))\n",
    "    input_stride = int(round(sampling_rate*frame_stride))\n",
    "    \n",
    "    trim_data = data.apply(lambda x : librosa.effects.trim(x, top_db=top_db,\n",
    "                                                           frame_length=input_nfft,\n",
    "                                                           hop_length=input_stride)[0])\n",
    "    return trim_data\n",
    "\n",
    "train_df[\"data\"] = trim_wav(train_df[\"data\"], top_db)\n",
    "test_df[\"data\"] = trim_wav(test_df[\"data\"], top_db)\n",
    "\n",
    "sns.histplot(train_df[\"data\"].apply(lambda x : len(x)))\n",
    "plt.show()\n",
    "sns.histplot(test_df[\"data\"].apply(lambda x : len(x)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_wav(x, reqlen=156027):\n",
    "    x_len = x.shape[0]\n",
    "    if reqlen < x_len:\n",
    "        max_offset = x_len - reqlen\n",
    "        offset = np.random.randint(max_offset)\n",
    "        x = x[offset:(reqlen+offset)]\n",
    "        return x\n",
    "    elif reqlen == x_len:\n",
    "        return x\n",
    "    else:\n",
    "        total_diff = reqlen - x_len\n",
    "        offset = np.random.randint(total_diff)\n",
    "        left_pad = offset\n",
    "        right_pad = total_diff - offset\n",
    "        return np.pad(x, (left_pad, right_pad), 'wrap')\n",
    "    \n",
    "train_df[\"data\"] = train_df[\"data\"].apply(lambda x : padding_wav(x))\n",
    "test_df[\"data\"] = test_df[\"data\"].apply(lambda x : padding_wav(x))\n",
    "    \n",
    "train_df[\"data\"].apply(lambda x : len(x)).min(), train_df[\"data\"].apply(lambda x : len(x)).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2744,
     "status": "ok",
     "timestamp": 1655181967331,
     "user": {
      "displayName": "‍김태형[ 대학원석·박사통합과정재학 / 산업경영공학과 ]",
      "userId": "00288066936238655028"
     },
     "user_tz": -540
    },
    "id": "QhMtoag0S8OM",
    "outputId": "2ac9cf2d-01a5-4205-f921-52e5ca8e4911"
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset(data):\n",
    "\n",
    "    frame_length = 0.025\n",
    "    frame_stride = 0.010\n",
    "\n",
    "    input_nfft = int(round(sampling_rate*frame_length))\n",
    "    input_stride = int(round(sampling_rate*frame_stride))\n",
    "\n",
    "    extracted_features = []\n",
    "    for i in data:\n",
    "        \n",
    "        if feature == \"mfcc\":\n",
    "            n_feature = 40\n",
    "            S = librosa.feature.mfcc(y=i,\n",
    "                                     sr=sampling_rate,\n",
    "                                     n_mfcc=n_feature,\n",
    "                                     n_fft=input_nfft,\n",
    "                                     hop_length=input_stride)\n",
    "            S_delta = librosa.feature.delta(S)\n",
    "            S_delta2 = librosa.feature.delta(S, order=2)\n",
    "            \n",
    "        elif feature == \"melspec\":\n",
    "            n_feature = 128\n",
    "            S = librosa.feature.melspectrogram(y=i,\n",
    "                                               sr=sampling_rate,\n",
    "                                               n_mels=n_feature,\n",
    "                                               n_fft=input_nfft,\n",
    "                                               hop_length=input_stride)\n",
    "            S = librosa.power_to_db(S, ref=np.max)\n",
    "            S_delta = librosa.feature.delta(S)\n",
    "            S_delta2 = librosa.feature.delta(S, order=2)\n",
    "            \n",
    "        S = np.stack((S, S_delta, S_delta2), axis=2)\n",
    "        extracted_features.append(S)\n",
    "        \n",
    "    return np.array(extracted_features)\n",
    "\n",
    "X_test = preprocess_dataset(test_df[\"data\"])\n",
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_feature(df):\n",
    "    temp = df.copy()\n",
    "    temp[\"old\"] = temp[\"age\"].apply(lambda x : 1 if x>=65 else 0)\n",
    "    temp[\"young\"] = temp[\"age\"].apply(lambda x : 1 if x<5 else 0)\n",
    "    temp[\"gender_male\"] = temp[\"gender\"].apply(lambda x : 1 if x==\"male\" else 0)\n",
    "    temp[\"gender_female\"] = temp[\"gender\"].apply(lambda x : 1 if x==\"female\" else 0)\n",
    "    temp[\"gender_other\"] = temp[\"gender\"].apply(lambda x : 1 if x==\"other\" else 0)\n",
    "    temp[\"condition1\"] = temp[\"respiratory_condition\"] + temp[\"fever_or_muscle_pain\"]\n",
    "    temp[\"condition2\"] = temp[\"respiratory_condition\"] * temp[\"fever_or_muscle_pain\"]\n",
    "    temp = temp.drop([\"data\", \"id\", \"age\", \"gender\"], axis=1)\n",
    "    return temp\n",
    "\n",
    "X_test_tab = preprocess_feature(test_df).values\n",
    "X_test_tab.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_augment = Compose([\n",
    "    PitchShift(min_semitones=-4, max_semitones=4, p=0.5),\n",
    "])\n",
    "\n",
    "def spec_augment(spec, T=time_mask, F=freq_mask, time_mask_num=n_time_mask, freq_mask_num=n_freq_mask):\n",
    "    feat_size = spec.shape[0]\n",
    "    seq_len = spec.shape[1]\n",
    "    # freq mask\n",
    "    for _ in range(freq_mask_num):\n",
    "        f = np.random.uniform(low=0.0, high=F)\n",
    "        f = int(f)\n",
    "        f0 = random.randint(0, feat_size - f)\n",
    "        spec[:, f0 : f0 + f] = 0\n",
    "    # time mask\n",
    "    for _ in range(time_mask_num):\n",
    "        t = np.random.uniform(low=0.0, high=T)\n",
    "        t = int(t)\n",
    "        t0 = random.randint(0, seq_len - t)\n",
    "        spec[t0 : t0 + t] = 0\n",
    "    return spec\n",
    "\n",
    "def augmentation(data):\n",
    "    temp = data.copy()\n",
    "    aug = []\n",
    "    for i in temp:\n",
    "        aug_ = spec_augment(i)\n",
    "        aug.append(aug_)\n",
    "    aug = np.array(aug)\n",
    "    return aug"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mtlHyQhgAoN"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def se_block(input_feature, ratio=8):\n",
    "    \n",
    "    channel_axis = 1 if K.image_data_format() == \"channels_first\" else -1\n",
    "    channel = input_feature.shape[channel_axis]\n",
    "    \n",
    "    se_feature = layers.GlobalAveragePooling2D()(input_feature)\n",
    "    se_feature = layers.Reshape((1, 1, channel))(se_feature)\n",
    "    se_feature = layers.Dense(channel // ratio,\n",
    "                              activation='relu',\n",
    "                              kernel_initializer='he_normal',\n",
    "                              use_bias=True,\n",
    "                              bias_initializer='zeros')(se_feature)\n",
    "    se_feature = layers.Dense(channel,\n",
    "                              activation='sigmoid',\n",
    "                              kernel_initializer='he_normal',\n",
    "                              use_bias=True,\n",
    "                              bias_initializer='zeros')(se_feature)\n",
    "    \n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        se_feature = layers.Permute((3, 1, 2))(se_feature)\n",
    "        \n",
    "    se_feature = layers.multiply([input_feature, se_feature])\n",
    "    \n",
    "    return se_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline():\n",
    "    \n",
    "    inp = tf.keras.Input(shape=(X_test.shape[1], X_test.shape[2], 3))\n",
    "    tab = tf.keras.Input(shape=(X_test_tab.shape[1],))\n",
    "    y = layers.Dense(16, use_bias=False)(tab)\n",
    "    \n",
    "    x = layers.Conv2D(32, 3, 1, padding=\"same\", kernel_initializer=\"he_normal\")(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(32, 3, 1, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    res = layers.Conv2D(64, 1, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    res = layers.BatchNormalization()(res)\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, 1, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(64, 3, 1, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, res])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    res = layers.Conv2D(128, 1, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    res = layers.BatchNormalization()(res)\n",
    "    \n",
    "    x = layers.Conv2D(128, 3, 1, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(128, 3, 1, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, res])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    res = layers.Conv2D(256, 1, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    res = layers.BatchNormalization()(res)\n",
    "    \n",
    "    x = layers.Conv2D(256, 3, 1, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(256, 3, 1, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, res])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    x = layers.MaxPooling2D()(x)\n",
    "    \n",
    "    res = layers.Conv2D(512, 1, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    res = layers.BatchNormalization()(res)\n",
    "    \n",
    "    x = layers.Conv2D(512, 3, 1, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = layers.Conv2D(512, 3, 1, padding=\"same\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Add()([x, res])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    x = se_block(x)\n",
    "    \n",
    "    x = layers.Permute((2, 1, 3))(x)\n",
    "    x = layers.Reshape((x.shape[1], -1))(x)\n",
    "    \n",
    "    x = layers.Bidirectional(layers.LSTM(64, dropout=0.2, return_sequences=True))(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, dropout=0.2))(x)\n",
    "    x = layers.LayerNormalization()(x)\n",
    "    \n",
    "    x = layers.Reshape((-1, 1))(x)\n",
    "    x = layers.Multiply()([x, y])\n",
    "    x = layers.Flatten()(x)\n",
    "    \n",
    "    x = layers.Dense(512, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(32, activation=\"relu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    oup = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "    \n",
    "    model = tf.keras.Model(inputs=[inp, tab], outputs=oup)\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model = baseline()    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_acc = []\n",
    "val_precision = []\n",
    "val_recall = []\n",
    "val_micro_f1 = []\n",
    "val_macro_f1 = []\n",
    "val_auroc = []\n",
    "val_aupr = []\n",
    "\n",
    "thresholds = []\n",
    "predictions = []\n",
    "\n",
    "idx=0\n",
    "\n",
    "skf = StratifiedKFold(n_splits=cv)\n",
    "for train_index, val_index in tqdm(skf.split(train_df, train_df[\"covid19\"])):\n",
    "    \n",
    "    idx+=1\n",
    "\n",
    "    X_train, X_val = train_df[\"data\"][train_index], train_df[\"data\"][val_index]\n",
    "    y_train, y_val = train_df[\"covid19\"][train_index], train_df[\"covid19\"][val_index]\n",
    "    \n",
    "    if wav_aug == True:\n",
    "        X_train = wav_augment(X_train.values, sampling_rate)\n",
    "        print(\"wav_aug successed\")\n",
    "    \n",
    "    X_tab = preprocess_feature(train_df).drop(\"covid19\", axis=1).values\n",
    "    X_train_tab = X_tab[train_index]\n",
    "    X_val_tab = X_tab[val_index]\n",
    "        \n",
    "    X_train = preprocess_dataset(X_train)\n",
    "    X_val = preprocess_dataset(X_val)\n",
    "    \n",
    "    if spec_aug == True:\n",
    "        X_train = augmentation(X_train)\n",
    "        print(\"spec_aug successed\")\n",
    "    \n",
    "    train_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices(((X_train, X_train_tab), y_train))\n",
    "        .shuffle(len(X_train))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    val_ds = (\n",
    "        tf.data.Dataset.from_tensor_slices(((X_val, X_val_tab), y_val))\n",
    "        .batch(BATCH_SIZE)\n",
    "        .prefetch(tf.data.experimental.AUTOTUNE)\n",
    "    )\n",
    "\n",
    "    model = baseline()\n",
    "\n",
    "    lr = tf.keras.optimizers.schedules.CosineDecay(learning_rate, decay_steps=1000)\n",
    "    if optimizer == \"adam\":\n",
    "        optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "    elif optimizer == \"sgd\":\n",
    "        optim = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "        \n",
    "    if loss == \"bc\":\n",
    "        label_smoothing=0\n",
    "        loss_function = tf.keras.losses.BinaryCrossentropy(label_smoothing=label_smoothing)\n",
    "    elif loss == \"fl\":\n",
    "        loss_function = tfa.losses.SigmoidFocalCrossEntropy()\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optim,\n",
    "        loss=loss_function,\n",
    "        metrics=[tf.keras.metrics.AUC(curve=\"ROC\", name='auroc')]\n",
    "    )\n",
    "    \n",
    "    checkpoint_filepath=f\"load_model/{parser.description}_{idx}\"\n",
    "\n",
    "    checkpoint_callback = [\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor='val_auroc',\n",
    "            patience=5,\n",
    "            mode='max',\n",
    "            restore_best_weights=True,\n",
    "        ),\n",
    "        tf.keras.callbacks.ModelCheckpoint(\n",
    "            checkpoint_filepath,\n",
    "            monitor='val_auroc',\n",
    "            save_best_only=True,\n",
    "            save_weights_only=True,\n",
    "            mode='max',\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    history = model.fit(\n",
    "        train_ds,\n",
    "        validation_data=val_ds,\n",
    "        epochs=EPOCHS,\n",
    "        callbacks=[checkpoint_callback, WandbCallback()],\n",
    "    )\n",
    "\n",
    "    max_f1 = 0\n",
    "    threshold = 0\n",
    "    for temp_threshold in np.linspace(0.05, 0.95, 37):\n",
    "        temp_f1 = f1_score(y_val, np.where(model.predict(val_ds)>temp_threshold, 1, 0), average=\"macro\")\n",
    "        if temp_f1 > max_f1:\n",
    "            max_f1 = temp_f1\n",
    "            threshold = temp_threshold\n",
    "    \n",
    "    print(f\"idx:{idx}, macro-f1:{max_f1}, threshold:{threshold}\")\n",
    "\n",
    "    val_acc.append(accuracy_score(y_val, np.where(model.predict(val_ds)>threshold, 1, 0)))\n",
    "    val_precision.append(precision_score(y_val, np.where(model.predict(val_ds)>threshold, 1, 0)))\n",
    "    val_recall.append(recall_score(y_val, np.where(model.predict(val_ds)>threshold, 1, 0)))\n",
    "    val_micro_f1.append(f1_score(y_val, np.where(model.predict(val_ds)>threshold, 1, 0), average=\"micro\"))\n",
    "    val_macro_f1.append(max_f1)\n",
    "    val_auroc.append(roc_auc_score(y_val, model.predict(val_ds)))\n",
    "    val_aupr.append(average_precision_score(y_val, model.predict(val_ds), pos_label=0))     \n",
    "     \n",
    "    thresholds.append(threshold)\n",
    "    predictions.append(model.predict([X_test, X_test_tab]))\n",
    "\n",
    "val_acc_mean = np.mean(val_acc, axis=0)\n",
    "val_precision_mean = np.mean(val_precision, axis=0)\n",
    "val_recall_mean = np.mean(val_recall, axis=0)\n",
    "val_micro_f1_mean = np.mean(val_micro_f1, axis=0)\n",
    "val_macro_f1_mean = np.mean(val_macro_f1, axis=0)\n",
    "val_auroc_mean = np.mean(val_auroc, axis=0)\n",
    "val_aupr_mean = np.mean(val_aupr, axis=0)\n",
    "\n",
    "print(f\"validation_accuracy:{val_acc_mean}, validation_precision:{val_precision_mean}, validation_recall:{val_recall_mean}, validation_micro-f1:{val_micro_f1_mean}\")\n",
    "print(f\"validation_macro-f1:{val_macro_f1_mean}, validation_auroc:{val_auroc_mean}, validation_aupr:{val_aupr_mean}\")\n",
    "\n",
    "wandb.log({\n",
    "    'validation_accuracy': val_acc_mean,\n",
    "    'validation_precision': val_precision_mean,\n",
    "    'validation_recall': val_recall_mean,\n",
    "    'validation_micro-f1': val_micro_f1_mean,\n",
    "    'validation_macro-f1': val_macro_f1_mean,\n",
    "    'validation_auroc': val_auroc_mean,\n",
    "    'validation_aupr': val_aupr_mean\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X7vNprtvVIb0"
   },
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ensemble==\"hard\":\n",
    "    temp=[]\n",
    "    for i, v in enumerate(thresholds):\n",
    "        temp.append(np.where(predictions[i]>v, 1, 0))\n",
    "    test_df[\"covid19\"] = np.where(np.hstack(temp).sum(axis=1)>cv/2, 1, 0)\n",
    "\n",
    "elif ensemble==\"soft\":\n",
    "    threshold = np.mean(thresholds)\n",
    "    test_df[\"covid19\"] = np.where(np.mean(predictions, axis=0)>threshold, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rb1j87j7k3l9"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['covid19'] = test_df['covid19']\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "test_df['covid19'].value_counts(normalize=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOI2a3K41SRHmr0VYqCJIVd",
   "collapsed_sections": [],
   "mount_file_id": "12SyNXKXsgbk5c855KhTFRgLEogv2xCNZ",
   "name": "2. Baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
