{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112690,
     "status": "ok",
     "timestamp": 1645972270802,
     "user": {
      "displayName": "‍김태형[ 대학원석·박사통합과정재학 / 산업경영공학과 ]",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFNpnkjN-Em0rOki5hhy0HR7yGAbxSpzCjHV0A=s64",
      "userId": "00288066936238655028"
     },
     "user_tz": -540
    },
    "id": "A1IbqGhzB7fy",
    "outputId": "fd656f36-afcb-4871-ee1c-557fd033af76"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import argparse\n",
    "# import wandb\n",
    "# from wandb.keras import WandbCallback\n",
    "# wandb.init(project=\"DACON_235900\", name=\"Baseline\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Baseline')\n",
    "parser.add_argument('--max_length', default=128, type=int)\n",
    "parser.add_argument('--optimizer', default=\"sgd\", type=str) # sgd or adam\n",
    "parser.add_argument('--learning_rate', default=0.01, type=float)\n",
    "parser.add_argument('--batch_size', default=32, type=int)\n",
    "parser.add_argument('--epochs', default=100, type=int)\n",
    "parser.add_argument('--validation_split', default=0.2, type=float)\n",
    "parser.add_argument('--seed', default=1011, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "# wandb.config.update(args)\n",
    "\n",
    "MAX_LENGTH = args.max_length\n",
    "BATCH_SIZE=args.batch_size\n",
    "EPOCHS=args.epochs\n",
    "VALIDATION_SPLIT=args.validation_split\n",
    "SEED=args.seed\n",
    "\n",
    "# lr = tf.keras.optimizers.schedules.CosineDecay(args.learning_rate, decay_steps=1000)\n",
    "# if args.optimizer == \"sgd\":\n",
    "#     optim = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.95)\n",
    "# elif args.optimizer == \"adam\":\n",
    "#     optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code1</th>\n",
       "      <th>code2</th>\n",
       "      <th>similar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flag = \"go\"\\ncnt = 0\\nwhile flag == \"go\":\\n   ...</td>\n",
       "      <td># Python 3+\\n#--------------------------------...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b, c = map(int, input().split())\\n\\nprint(b * c)</td>\n",
       "      <td>import numpy as np\\n\\nn = int(input())\\na = np...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>import numpy as np\\nimport sys\\nread = sys.std...</td>\n",
       "      <td>N, M = map(int, input().split())\\nif M%2 != 0:...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b, c = map(int, input().split())\\n\\nprint(b * c)</td>\n",
       "      <td>n,m=map(int,input().split())\\nh=list(map(int,i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>s=input()\\nt=input()\\nans=0\\nfor i in range(le...</td>\n",
       "      <td>import math\\na,b,h,m=map(int,input().split())\\...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               code1  \\\n",
       "0  flag = \"go\"\\ncnt = 0\\nwhile flag == \"go\":\\n   ...   \n",
       "1   b, c = map(int, input().split())\\n\\nprint(b * c)   \n",
       "2  import numpy as np\\nimport sys\\nread = sys.std...   \n",
       "3   b, c = map(int, input().split())\\n\\nprint(b * c)   \n",
       "4  s=input()\\nt=input()\\nans=0\\nfor i in range(le...   \n",
       "\n",
       "                                               code2  similar  \n",
       "0  # Python 3+\\n#--------------------------------...        1  \n",
       "1  import numpy as np\\n\\nn = int(input())\\na = np...        0  \n",
       "2  N, M = map(int, input().split())\\nif M%2 != 0:...        0  \n",
       "3  n,m=map(int,input().split())\\nh=list(map(int,i...        0  \n",
       "4  import math\\na,b,h,m=map(int,input().split())\\...        0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/sample_train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 17970 entries, 0 to 17969\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   code1    17970 non-null  object\n",
      " 1   code2    17970 non-null  object\n",
      " 2   similar  17970 non-null  int64 \n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 421.3+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    9005\n",
       "0    8965\n",
       "Name: similar, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[\"similar\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pair_id</th>\n",
       "      <th>code1</th>\n",
       "      <th>code2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>def main():\\n  s = input()\\n  if s.count('a') ...</td>\n",
       "      <td>N,K = map(int,input().split())\\nA = list(map(i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>N,K,Q = map(int,input().split())\\npoints = [0]...</td>\n",
       "      <td>N, K, Q = map(int,input().split())\\n\\nif K &gt; Q...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>from itertools import combinations\\nn = int(in...</td>\n",
       "      <td>s = input()\\nt = input()\\nlength_s = len(s)\\nl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>a,b=map(int,input().split())\\n\\nans1=a+b\\nans2...</td>\n",
       "      <td>a, b, c, d = map(int,input().split())\\n\\nif a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>S = input()\\nK = int(input())\\n\\nind = -1\\nfor...</td>\n",
       "      <td>H, W = map(int, input().split())\\ngrid = []\\nf...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pair_id                                              code1  \\\n",
       "0        1  def main():\\n  s = input()\\n  if s.count('a') ...   \n",
       "1        2  N,K,Q = map(int,input().split())\\npoints = [0]...   \n",
       "2        3  from itertools import combinations\\nn = int(in...   \n",
       "3        4  a,b=map(int,input().split())\\n\\nans1=a+b\\nans2...   \n",
       "4        5  S = input()\\nK = int(input())\\n\\nind = -1\\nfor...   \n",
       "\n",
       "                                               code2  \n",
       "0  N,K = map(int,input().split())\\nA = list(map(i...  \n",
       "1  N, K, Q = map(int,input().split())\\n\\nif K > Q...  \n",
       "2  s = input()\\nt = input()\\nlength_s = len(s)\\nl...  \n",
       "3  a, b, c, d = map(int,input().split())\\n\\nif a ...  \n",
       "4  H, W = map(int, input().split())\\ngrid = []\\nf...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"data/test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 179700 entries, 0 to 179699\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   pair_id  179700 non-null  int64 \n",
      " 1   code1    179700 non-null  object\n",
      " 2   code2    179700 non-null  object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 4.1+ MB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179700"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test[\"pair_id\"].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code_folder = 'data/code'\n",
    "# problem_folders = os.listdir(code_folder)\n",
    "# len(problem_folders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_script(script):\n",
    "#     '''\n",
    "#     간단한 전처리 함수\n",
    "#     주석 -> 삭제\n",
    "#     '    '-> tab 변환\n",
    "#     다중 개행 -> 한 번으로 변환\n",
    "#     '''\n",
    "#     with open(script,'r',encoding='utf-8') as file:\n",
    "#         lines = file.readlines()\n",
    "#         preproc_lines = []\n",
    "#         for line in lines:\n",
    "#             if line.lstrip().startswith('#'):\n",
    "#                 continue\n",
    "#             line = line.rstrip()\n",
    "#             if '#' in line:\n",
    "#                 line = line[:line.index('#')]\n",
    "#             line = line.replace('\\n','')\n",
    "#             line = line.replace('    ','\\t')\n",
    "#             if line == '':\n",
    "#                 continue\n",
    "#             preproc_lines.append(line)\n",
    "#         preprocessed_script = '\\n'.join(preproc_lines)\n",
    "#     return preprocessed_script\n",
    "\n",
    "# preproc_scripts = []\n",
    "# problem_nums = []\n",
    "\n",
    "# for problem_folder in tqdm(problem_folders):\n",
    "#     scripts = os.listdir(os.path.join(code_folder,problem_folder))\n",
    "#     problem_num = scripts[0].split('_')[0]\n",
    "#     for script in scripts:\n",
    "#         script_file = os.path.join(code_folder,problem_folder,script)\n",
    "#         preprocessed_script = preprocess_script(script_file)\n",
    "\n",
    "#         preproc_scripts.append(preprocessed_script)\n",
    "#     problem_nums.extend([problem_num]*len(scripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(problem_nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame({'code': preproc_scripts,\n",
    "#                    'problem_num': problem_nums})\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoTokenizer\n",
    "\n",
    "# # tokenizer = AutoTokenizer.from_pretrained(\"microsoft/graphcodebert-base\")\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# df['tokens'] = df['code'].apply(tokenizer.tokenize)\n",
    "# df['len'] = df['tokens'].apply(len)\n",
    "# df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndf = df[df['len'] <= 256].reset_index(drop=True)\n",
    "# ndf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ndf['problem_num'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_df, valid_df, train_label, valid_label = train_test_split(\n",
    "#     ndf,\n",
    "#     ndf['problem_num'],\n",
    "#     test_size=VALIDATION_SPLIT,\n",
    "#     random_state=SEED,\n",
    "#     stratify=ndf['problem_num'],\n",
    "# )\n",
    "\n",
    "# train_df = train_df.reset_index(drop=True)\n",
    "# valid_df = valid_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rank_bm25 import BM25Okapi\n",
    "# from itertools import combinations\n",
    "\n",
    "# codes = train_df['code'].to_list()\n",
    "# problems = train_df['problem_num'].unique().tolist()\n",
    "# problems.sort()\n",
    "\n",
    "# tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "# bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# total_positive_pairs = []\n",
    "# total_negative_pairs = []\n",
    "\n",
    "# for problem in tqdm(problems):\n",
    "#     solution_codes = train_df[train_df['problem_num'] == problem]['code']\n",
    "#     positive_pairs = list(combinations(solution_codes.to_list(),2))\n",
    "\n",
    "#     solution_codes_indices = solution_codes.index.to_list()\n",
    "#     negative_pairs = []\n",
    "\n",
    "#     first_tokenized_code = tokenizer.tokenize(positive_pairs[0][0])\n",
    "#     negative_code_scores = bm25.get_scores(first_tokenized_code)\n",
    "#     negative_code_ranking = negative_code_scores.argsort()[::-1] # 내림차순\n",
    "#     ranking_idx = 0\n",
    "    \n",
    "#     for solution_code in solution_codes:\n",
    "#         negative_solutions = []\n",
    "#         while len(negative_solutions) < len(positive_pairs) // len(solution_codes):\n",
    "#             high_score_idx = negative_code_ranking[ranking_idx]\n",
    "            \n",
    "#             if high_score_idx not in solution_codes_indices:\n",
    "#                 negative_solutions.append(train_df['code'].iloc[high_score_idx])\n",
    "#             ranking_idx += 1\n",
    "\n",
    "#         for negative_solution in negative_solutions:\n",
    "#             negative_pairs.append((solution_code, negative_solution))\n",
    "    \n",
    "#     total_positive_pairs.extend(positive_pairs)\n",
    "#     total_negative_pairs.extend(negative_pairs)\n",
    "\n",
    "# pos_code1 = list(map(lambda x:x[0],total_positive_pairs))\n",
    "# pos_code2 = list(map(lambda x:x[1],total_positive_pairs))\n",
    "\n",
    "# neg_code1 = list(map(lambda x:x[0],total_negative_pairs))\n",
    "# neg_code2 = list(map(lambda x:x[1],total_negative_pairs))\n",
    "\n",
    "# pos_label = [1]*len(pos_code1)\n",
    "# neg_label = [0]*len(neg_code1)\n",
    "\n",
    "# pos_code1.extend(neg_code1)\n",
    "# total_code1 = pos_code1\n",
    "# pos_code2.extend(neg_code2)\n",
    "# total_code2 = pos_code2\n",
    "# pos_label.extend(neg_label)\n",
    "# total_label = pos_label\n",
    "# pair_data = pd.DataFrame(data={\n",
    "#     'code1':total_code1,\n",
    "#     'code2':total_code2,\n",
    "#     'similar':total_label\n",
    "# })\n",
    "# pair_data = pair_data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# pair_data.to_csv('train_data.csv', index=False)\n",
    "\n",
    "# pair_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pair_data = pd.read_csv('train_data.csv')\n",
    "# pair_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BaselineModel():\n",
    "#     def __init__(self, threshold=0.5):\n",
    "#         super(BaselineModel, self).__init__()\n",
    "#         self.threshold = threshold \n",
    "#         self.vectorizer = CountVectorizer()\n",
    "#     def fit(self, code1, code2):\n",
    "#         self.vectorizer.fit(code1)\n",
    "#         self.vectorizer.fit(code2)\n",
    "#         print('Done.')\n",
    "#     def predict_proba(self, code1, code2):\n",
    "#         code1_vecs = self.vectorizer.transform(code1)\n",
    "#         code2_vecs = self.vectorizer.transform(code2)\n",
    "#         preds = []\n",
    "#         for code1_vec, code2_vec in zip(code1_vecs, code2_vecs):\n",
    "#             preds.append(cosine_similarity(code1_vec, code2_vec))\n",
    "#         preds = np.reshape(preds, len(preds))\n",
    "#         print('Done.')\n",
    "#         return preds\n",
    "#     def predict(self, code1, code2):\n",
    "#         preds = self.predict_proba(code1, code2)\n",
    "#         preds = np.where(preds>self.threshold, 1, 0)\n",
    "#         return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertSemanticDataGenerator(tf.keras.utils.Sequence):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        sentence_pairs,\n",
    "        labels=[0, 1],\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        include_targets=True,\n",
    "    ):\n",
    "        self.sentence_pairs = sentence_pairs\n",
    "        self.labels = labels\n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.include_targets = include_targets\n",
    "        # Load our BERT Tokenizer to encode the text.\n",
    "        # We will use base-base-uncased pretrained model.\n",
    "        self.tokenizer = transformers.BertTokenizer.from_pretrained(\n",
    "            \"bert-base-uncased\", do_lower_case=True\n",
    "        )\n",
    "        self.indexes = np.arange(len(self.sentence_pairs))\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        # Denotes the number of batches per epoch.\n",
    "        return len(self.sentence_pairs) // self.batch_size\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieves the batch of index.\n",
    "        indexes = self.indexes[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
    "        sentence_pairs = self.sentence_pairs[indexes]\n",
    "\n",
    "        # With BERT tokenizer's batch_encode_plus batch of both the sentences are\n",
    "        # encoded together and separated by [SEP] token.\n",
    "        encoded = self.tokenizer.batch_encode_plus(\n",
    "            sentence_pairs.tolist(),\n",
    "            add_special_tokens=True,\n",
    "            max_length=MAX_LENGTH,\n",
    "            return_attention_mask=True,\n",
    "            return_token_type_ids=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"tf\",\n",
    "        )\n",
    "\n",
    "        # Convert batch of encoded features to numpy array.\n",
    "        input_ids = np.array(encoded[\"input_ids\"], dtype=\"int32\")\n",
    "        attention_masks = np.array(encoded[\"attention_mask\"], dtype=\"int32\")\n",
    "        token_type_ids = np.array(encoded[\"token_type_ids\"], dtype=\"int32\")\n",
    "\n",
    "        # Set to true if data generator is used for training/validation.\n",
    "        if self.include_targets:\n",
    "            labels = np.array(self.labels[indexes], dtype=\"int32\")\n",
    "            return [input_ids, attention_masks, token_type_ids], labels\n",
    "        else:\n",
    "            return [input_ids, attention_masks, token_type_ids]\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        # Shuffle indexes after each epoch if shuffle is set to True.\n",
    "        if self.shuffle:\n",
    "            np.random.RandomState(SEED).shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_ids (InputLayer)         [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " attention_masks (InputLayer)   [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " token_type_ids (InputLayer)    [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]',              \n",
      "                                thPoolingAndCrossAt               'attention_masks[0][0]',        \n",
      "                                tentions(last_hidde               'token_type_ids[0][0]']         \n",
      "                                n_state=(None, 128,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " bidirectional (Bidirectional)  (None, 128, 128)     426496      ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " global_average_pooling1d (Glob  (None, 128)         0           ['bidirectional[0][0]']          \n",
      " alAveragePooling1D)                                                                              \n",
      "                                                                                                  \n",
      " global_max_pooling1d (GlobalMa  (None, 128)         0           ['bidirectional[0][0]']          \n",
      " xPooling1D)                                                                                      \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 256)          0           ['global_average_pooling1d[0][0]'\n",
      "                                                                 , 'global_max_pooling1d[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_37 (Dropout)           (None, 256)          0           ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 1)            257         ['dropout_37[0][0]']             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 109,908,993\n",
      "Trainable params: 426,753\n",
      "Non-trainable params: 109,482,240\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_ids = tf.keras.layers.Input(\n",
    "    shape=(MAX_LENGTH,), dtype=tf.int32, name=\"input_ids\"\n",
    ")\n",
    "# Attention masks indicates to the model which tokens should be attended to.\n",
    "attention_masks = tf.keras.layers.Input(\n",
    "    shape=(MAX_LENGTH,), dtype=tf.int32, name=\"attention_masks\"\n",
    ")\n",
    "# Token type ids are binary masks identifying different sequences in the model.\n",
    "token_type_ids = tf.keras.layers.Input(\n",
    "    shape=(MAX_LENGTH,), dtype=tf.int32, name=\"token_type_ids\"\n",
    ")\n",
    "# Loading pretrained BERT model.\n",
    "bert_model = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\n",
    "# Freeze the BERT model to reuse the pretrained features without modifying them.\n",
    "bert_model.trainable = False\n",
    "\n",
    "bert_output = bert_model(\n",
    "    input_ids, attention_mask=attention_masks, token_type_ids=token_type_ids\n",
    ")\n",
    "sequence_output = bert_output.last_hidden_state\n",
    "pooled_output = bert_output.pooler_output\n",
    "# Add trainable layers on top of frozen layers to adapt the pretrained features on the new data.\n",
    "bi_lstm = tf.keras.layers.Bidirectional(\n",
    "    tf.keras.layers.LSTM(64, return_sequences=True)\n",
    ")(sequence_output)\n",
    "# Applying hybrid pooling approach to bi_lstm sequence output.\n",
    "avg_pool = tf.keras.layers.GlobalAveragePooling1D()(bi_lstm)\n",
    "max_pool = tf.keras.layers.GlobalMaxPooling1D()(bi_lstm)\n",
    "concat = tf.keras.layers.concatenate([avg_pool, max_pool])\n",
    "dropout = tf.keras.layers.Dropout(0.3)(concat)\n",
    "output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(dropout)\n",
    "model = tf.keras.models.Model(\n",
    "    inputs=[input_ids, attention_masks, token_type_ids], outputs=output\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similar</th>\n",
       "      <th>len1</th>\n",
       "      <th>len2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>17970.000000</td>\n",
       "      <td>17970.000000</td>\n",
       "      <td>17970.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.501113</td>\n",
       "      <td>159.671508</td>\n",
       "      <td>164.177073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500013</td>\n",
       "      <td>178.446994</td>\n",
       "      <td>232.938394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>64.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>110.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>202.000000</td>\n",
       "      <td>203.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>7483.000000</td>\n",
       "      <td>14271.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            similar          len1          len2\n",
       "count  17970.000000  17970.000000  17970.000000\n",
       "mean       0.501113    159.671508    164.177073\n",
       "std        0.500013    178.446994    232.938394\n",
       "min        0.000000     10.000000     10.000000\n",
       "25%        0.000000     64.000000     64.000000\n",
       "50%        1.000000    109.000000    110.000000\n",
       "75%        1.000000    202.000000    203.000000\n",
       "max        1.000000   7483.000000  14271.000000"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "train['tokens1'] = train['code1'].apply(tokenizer.tokenize)\n",
    "train['len1'] = train['tokens1'].apply(len)\n",
    "train['tokens2'] = train['code2'].apply(tokenizer.tokenize)\n",
    "train['len2'] = train['tokens2'].apply(len)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similar</th>\n",
       "      <th>len1</th>\n",
       "      <th>len2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>16916.000000</td>\n",
       "      <td>16916.000000</td>\n",
       "      <td>16916.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.499231</td>\n",
       "      <td>138.070939</td>\n",
       "      <td>138.490305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500014</td>\n",
       "      <td>103.186325</td>\n",
       "      <td>103.756053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>103.000000</td>\n",
       "      <td>104.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>187.000000</td>\n",
       "      <td>186.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>512.000000</td>\n",
       "      <td>512.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            similar          len1          len2\n",
       "count  16916.000000  16916.000000  16916.000000\n",
       "mean       0.499231    138.070939    138.490305\n",
       "std        0.500014    103.186325    103.756053\n",
       "min        0.000000     10.000000     10.000000\n",
       "25%        0.000000     63.000000     62.000000\n",
       "50%        0.000000    103.000000    104.000000\n",
       "75%        1.000000    187.000000    186.000000\n",
       "max        1.000000    512.000000    512.000000"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndf = train[(train['len1'] <= 512) & (train['len2'] <= 512)].reset_index(drop=True)\n",
    "ndf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13532, 2), (3384, 2), (13532,), (3384,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = ndf[[\"code1\", \"code2\"]]\n",
    "y = ndf[\"similar\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VALIDATION_SPLIT, random_state=SEED, stratify=y)\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pair_data[[\"code1\", \"code2\"]]\n",
    "# y = pair_data[\"similar\"]\n",
    "\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VALIDATION_SPLIT, random_state=SEED, stratify=y)\n",
    "\n",
    "# X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = BertSemanticDataGenerator(\n",
    "    X_train[[\"code1\", \"code2\"]].values.astype(\"str\"),\n",
    "    y_train.values,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    ")\n",
    "valid_data = BertSemanticDataGenerator(\n",
    "    X_val[[\"code1\", \"code2\"]].values.astype(\"str\"),\n",
    "    y_val.values,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Taehyeong\\anaconda3\\envs\\DACON_tf\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2285: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "422/422 [==============================] - 71s 140ms/step - loss: 0.4802 - acc: 0.7645 - val_loss: 0.3766 - val_acc: 0.8289\n",
      "Epoch 2/100\n",
      "422/422 [==============================] - 57s 134ms/step - loss: 0.3660 - acc: 0.8346 - val_loss: 0.3399 - val_acc: 0.8494\n",
      "Epoch 3/100\n",
      "422/422 [==============================] - 57s 136ms/step - loss: 0.3177 - acc: 0.8611 - val_loss: 0.2757 - val_acc: 0.8821\n",
      "Epoch 4/100\n",
      "422/422 [==============================] - ETA: 0s - loss: 0.2742 - acc: 0.8857"
     ]
    }
   ],
   "source": [
    "callback = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "    )\n",
    "]\n",
    "\n",
    "\n",
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=valid_data,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze the bert_model.\n",
    "bert_model.trainable = True\n",
    "# Recompile the model to make the change effective.\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    loss=\"binary_crossentropy\",\n",
    "    metrics=[\"acc\"],\n",
    ")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_data,\n",
    "    validation_data=valid_data,\n",
    "    epochs=EPOCHS,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_similarity(sentence1, sentence2):\n",
    "#     sentence_pairs = np.array([[str(sentence1), str(sentence2)]])\n",
    "#     test_data = BertSemanticDataGenerator(\n",
    "#         sentence_pairs, labels=None, batch_size=1, shuffle=False, include_targets=False,\n",
    "#     )\n",
    "\n",
    "#     proba = model.predict(test_data[0])[0]\n",
    "#     idx = np.argmax(proba)\n",
    "#     proba = f\"{proba[idx]: .2f}%\"\n",
    "#     pred = labels[idx]\n",
    "#     return pred, proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "class BaselineModel():\n",
    "    def __init__(self, threshold=0.5):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.threshold = threshold \n",
    "        self.vectorizer = HashingVectorizer()\n",
    "    def fit(self, code1, code2):\n",
    "        self.vectorizer.fit(code1)\n",
    "        self.vectorizer.fit(code2)\n",
    "        print('Done.')\n",
    "    def predict_proba(self, code1, code2):\n",
    "        code1_vecs = self.vectorizer.transform(code1)\n",
    "        code2_vecs = self.vectorizer.transform(code2)\n",
    "        preds = []\n",
    "        for code1_vec, code2_vec in zip(code1_vecs, code2_vecs):\n",
    "            preds.append(cosine_similarity(code1_vec, code2_vec))\n",
    "        preds = np.reshape(preds, len(preds))\n",
    "        print('Done.')\n",
    "        return preds\n",
    "    def predict(self, code1, code2):\n",
    "        preds = self.predict_proba(code1, code2)\n",
    "        preds = np.where(preds>self.threshold, 1, 0)\n",
    "        return preds\n",
    "    \n",
    "model = BaselineModel(threshold=0.5)\n",
    "model.fit(pair_data['code1'], pair_data['code2'])\n",
    "preds = model.predict(pair_data['code1'], pair_data['code2'])\n",
    "accuracy_score(pair_data[\"similar\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(threshold=0.5)\n",
    "model.fit(train['code1'], train['code2'])\n",
    "preds = model.predict(train['code1'], train['code2'])\n",
    "accuracy_score(train[\"similar\"], preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train[[\"code1\", \"code2\"]]\n",
    "y = train[\"similar\"]\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VALIDATION_SPLIT, random_state=SEED, stratify=y)\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=VALIDATION_SPLIT, random_state=SEED, stratify=y)\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(threshold=0.5)\n",
    "model.fit(X_train['code1'], X_train['code2'])\n",
    "preds = model.predict(X_val['code1'], X_val['code2'])\n",
    "accuracy_score(y_val, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BaselineModel(threshold=0.5)\n",
    "model.fit(train['code1'], train['code2'])\n",
    "preds = model.predict(test['code1'], test['code2'])\n",
    "np.sum(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "submission['similar'] = preds\n",
    "submission.to_csv(f\"{parser.description}.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.Xception(\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    "    input_shape=input_shape,\n",
    ")\n",
    "\n",
    "inp = tf.keras.Input(shape=input_shape)\n",
    "x = base_model(inp)\n",
    "# x = se_block(x)\n",
    "# x = cbam_block(x)\n",
    "# x = layers.Dropout(0.2)(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "oup = layers.Dense(len(np.unique(y)), activation=\"softmax\")(x)\n",
    "model = tf.keras.Model(inputs=inp, outputs=oup)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    .shuffle(len(X_train))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(lambda x, y: (tf.py_function(augment, [x], [tf.float32])[0], y),\n",
    "         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optim,\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "checkpoint_filepath=f\"load_model/{parser.description}\"\n",
    "\n",
    "checkpoint_callback = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[checkpoint_callback, WandbCallback()],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_filepath)  "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPxWDbXdBZDPv2XiwW8C1k0",
   "collapsed_sections": [],
   "mount_file_id": "13o4BpF8zzuXcEiNVlVG2KwTXMW1Y55_v",
   "name": "test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
