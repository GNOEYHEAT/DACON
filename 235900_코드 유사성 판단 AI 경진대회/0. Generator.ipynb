{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112690,
     "status": "ok",
     "timestamp": 1645972270802,
     "user": {
      "displayName": "‍김태형[ 대학원석·박사통합과정재학 / 산업경영공학과 ]",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFNpnkjN-Em0rOki5hhy0HR7yGAbxSpzCjHV0A=s64",
      "userId": "00288066936238655028"
     },
     "user_tz": -540
    },
    "id": "A1IbqGhzB7fy",
    "outputId": "fd656f36-afcb-4871-ee1c-557fd033af76"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "from itertools import combinations\n",
    "\n",
    "import re\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Generator')\n",
    "parser.add_argument('--pretrained_model', default=\"bert\", type=str) # bert or codebert\n",
    "parser.add_argument('--max_length', default=384, type=int)# 384 or 256\n",
    "parser.add_argument('--validation_split', default=0.2, type=float)\n",
    "parser.add_argument('--seed', default=1011, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "if args.pretrained_model == \"bert\":\n",
    "    pretrained_model = \"bert-base-uncased\"\n",
    "elif args.pretrained_model == \"codebert\":\n",
    "    pretrained_model = \"microsoft/graphcodebert-base\"\n",
    "    \n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model, do_lower_case=True)\n",
    "tokenizer.truncation_side='left'\n",
    "\n",
    "MAX_LENGTH = args.max_length\n",
    "VALIDATION_SPLIT=args.validation_split\n",
    "SEED=args.seed\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generating Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_code(code):\n",
    "    with open(code,'r',encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "        script = '\\n'.join(lines)\n",
    "    return script\n",
    "\n",
    "root_dir = 'data/code'\n",
    "\n",
    "problem_num_list=[]\n",
    "code_path_list = []\n",
    "possible_code_extension = ['.py']\n",
    "\n",
    "\n",
    "for (root, dirs, files) in os.walk(root_dir):\n",
    "    if len(files) > 0:\n",
    "        for file_name in files:\n",
    "            if os.path.splitext(file_name)[1] in possible_code_extension:\n",
    "                code_path = root + '/' + file_name\n",
    "                code_path_list.append(code_path)\n",
    "                problem_num_list.append(root.split(\"\\\\\")[-1])\n",
    "                \n",
    "code_list=[read_code(i) for i in tqdm(code_path_list)]\n",
    "\n",
    "len(code_list), len(problem_num_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(series):\n",
    "    temp = series.copy()\n",
    "    temp = temp.apply(lambda x : re.sub(re.compile(\"\\\"\\\"\\\".*\\\"\\\"\\\"\", re.DOTALL), \"\", x))\n",
    "    temp = temp.apply(lambda x : re.sub(re.compile(\"\\'\\'\\'.*\\'\\'\\'\", re.DOTALL), \"\", x))\n",
    "    temp = temp.apply(lambda x : re.sub(re.compile(\"#.*\"), \"\", x))\n",
    "    temp = temp.apply(lambda x : re.sub(\"print(.*)\", \"\", x))\n",
    "    temp = temp.apply(lambda x : x.replace(\"    \",\"\\t\"))\n",
    "    temp = temp.apply(lambda x : x.split(\"\\n\"))\n",
    "    temp_document=[]\n",
    "    for document in tqdm(temp):\n",
    "        temp_sentence=[]\n",
    "        for sentence in document:\n",
    "            if \"import\" in sentence:\n",
    "                continue\n",
    "            if sentence==\"\":\n",
    "                continue\n",
    "            temp_sentence.append(sentence.rstrip())\n",
    "        temp_document.append(\"\\n\".join(temp_sentence))\n",
    "    temp = pd.Series(data=temp_document, index=temp.index, name=temp.name)\n",
    "    return temp\n",
    "\n",
    "def code_preprocessing(df):\n",
    "    temp = df.copy()\n",
    "    temp[\"code\"] = text_preprocessing(temp[\"code\"])\n",
    "    # temp[\"len\"] = temp[\"code\"].apply(tokenizer.tokenize).apply(len)\n",
    "    # temp = temp[temp['len']<=MAX_LENGTH].reset_index(drop=True)\n",
    "    return temp\n",
    "\n",
    "df = pd.DataFrame({'code': code_list,\n",
    "                   'problem_num': problem_num_list})\n",
    "\n",
    "df = code_preprocessing(df)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(df):\n",
    "    \n",
    "    codes = df['code'].to_list()\n",
    "    problems = df['problem_num'].unique().tolist()\n",
    "    problems.sort()\n",
    "\n",
    "    tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "    bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "    total_positive_pairs = []\n",
    "    total_negative_pairs = []\n",
    "\n",
    "    for problem in tqdm(problems):\n",
    "        solution_codes = df[df['problem_num'] == problem]['code']\n",
    "        positive_pairs = list(combinations(solution_codes.to_list(),2))\n",
    "\n",
    "        solution_codes_indices = solution_codes.index.to_list()\n",
    "        negative_pairs = []\n",
    "\n",
    "        ### scoring\n",
    "        negative_code_scores = []\n",
    "        for solution_code in solution_codes:\n",
    "            tokenized_code = tokenizer.tokenize(solution_code)\n",
    "            negative_code_scores.append(bm25.get_scores(tokenized_code))\n",
    "        negative_code_scores = np.mean(negative_code_scores, axis=0)\n",
    "        negative_code_ranking = negative_code_scores.argsort()[::-1]\n",
    "        ranking_idx = 0\n",
    "\n",
    "        for solution_code in solution_codes:\n",
    "            negative_solutions = []\n",
    "            while len(negative_solutions) < len(positive_pairs) // len(solution_codes):\n",
    "                high_score_idx = negative_code_ranking[ranking_idx]\n",
    "\n",
    "                if high_score_idx not in solution_codes_indices:\n",
    "                    negative_solutions.append(df['code'].iloc[high_score_idx])\n",
    "                ranking_idx += 1\n",
    "\n",
    "            for negative_solution in negative_solutions:\n",
    "                negative_pairs.append((solution_code, negative_solution))\n",
    "\n",
    "        total_positive_pairs.extend(positive_pairs)\n",
    "        total_negative_pairs.extend(negative_pairs)\n",
    "    \n",
    "    pos_code1 = list(map(lambda x:x[0],total_positive_pairs))\n",
    "    pos_code2 = list(map(lambda x:x[1],total_positive_pairs))\n",
    "\n",
    "    neg_code1 = list(map(lambda x:x[0],total_negative_pairs))\n",
    "    neg_code2 = list(map(lambda x:x[1],total_negative_pairs))\n",
    "\n",
    "    pos_label = [1]*len(pos_code1)\n",
    "    neg_label = [0]*len(neg_code1)\n",
    "\n",
    "    pos_code1.extend(neg_code1)\n",
    "    total_code1 = pos_code1\n",
    "    pos_code2.extend(neg_code2)\n",
    "    total_code2 = pos_code2\n",
    "    pos_label.extend(neg_label)\n",
    "    total_label = pos_label\n",
    "    pair_data = pd.DataFrame(data={\n",
    "        'code1':total_code1,\n",
    "        'code2':total_code2,\n",
    "        'similar':total_label\n",
    "    })\n",
    "    return pair_data   \n",
    "    \n",
    "\n",
    "train_df, valid_df, train_label, valid_label = train_test_split(\n",
    "    df,\n",
    "    df['problem_num'],\n",
    "    test_size=VALIDATION_SPLIT,\n",
    "    random_state=SEED,\n",
    "    stratify=df['problem_num'],\n",
    ")\n",
    "\n",
    "train_df = generator(train_df.reset_index(drop=True))\n",
    "valid_df = generator(valid_df.reset_index(drop=True))\n",
    "\n",
    "len(train_df), len(valid_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSV Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# train_df.to_csv(f'train_{args.pretrained_model}.csv', index=False)\n",
    "# valid_df.to_csv(f'valid_{args.pretrained_model}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPxWDbXdBZDPv2XiwW8C1k0",
   "collapsed_sections": [],
   "mount_file_id": "13o4BpF8zzuXcEiNVlVG2KwTXMW1Y55_v",
   "name": "test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
