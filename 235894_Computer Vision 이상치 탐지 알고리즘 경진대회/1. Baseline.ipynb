{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112690,
     "status": "ok",
     "timestamp": 1645972270802,
     "user": {
      "displayName": "‍김태형[ 대학원석·박사통합과정재학 / 산업경영공학과 ]",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFNpnkjN-Em0rOki5hhy0HR7yGAbxSpzCjHV0A=s64",
      "userId": "00288066936238655028"
     },
     "user_tz": -540
    },
    "id": "A1IbqGhzB7fy",
    "outputId": "fd656f36-afcb-4871-ee1c-557fd033af76"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import argparse\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"DACON_235894\", name=\"Baseline\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Baseline')\n",
    "parser.add_argument('--resize_size', default=224, type=int)\n",
    "parser.add_argument('--optimizer', default=\"adam\", type=str) # adam or sgd\n",
    "parser.add_argument('--learning_rate', default=0.0003, type=float)\n",
    "parser.add_argument('--loss', default=\"cc\", type=str) # cc or fl\n",
    "parser.add_argument('--label_smoothing', default=0, type=float) # 0 or 0.1\n",
    "parser.add_argument('--class_weight', default=False, type=bool) # False or True\n",
    "parser.add_argument('--batch_size', default=32, type=int)\n",
    "parser.add_argument('--epochs', default=100, type=int)\n",
    "parser.add_argument('--validation_split', default=0.2, type=float)\n",
    "parser.add_argument('--seed', default=1011, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "wandb.config.update(args)\n",
    "\n",
    "resize_size=args.resize_size\n",
    "BATCH_SIZE=args.batch_size\n",
    "EPOCHS=args.epochs\n",
    "VALIDATION_SPLIT=args.validation_split\n",
    "SEED=args.seed\n",
    "\n",
    "if args.optimizer == \"adam\":\n",
    "    lr = tf.keras.optimizers.schedules.CosineDecay(args.learning_rate, decay_steps=1000)\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "elif args.optimizer == \"sgd\":\n",
    "    optim = tf.keras.optimizers.SGD(learning_rate=args.learning_rate, momentum=0.9)\n",
    "\n",
    "if args.loss == \"cc\":\n",
    "    loss_function = tf.keras.losses.CategoricalCrossentropy(label_smoothing=args.label_smoothing)\n",
    "elif args.loss == \"fl\":\n",
    "    loss_function = tfa.losses.SigmoidFocalCrossEntropy()\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_load(path):\n",
    "    img = cv2.imread(path)[:,:,::-1]\n",
    "    img = tf.image.central_crop(img, 0.9).numpy()\n",
    "    img = cv2.resize(img, (resize_size, resize_size), cv2.INTER_AREA)\n",
    "    return img\n",
    "\n",
    "train_png = sorted(glob('raw_data/train/*.png'))\n",
    "test_png = sorted(glob('raw_data/test/*.png'))\n",
    "\n",
    "train_imgs = [img_load(m) for m in tqdm(train_png)]\n",
    "test_imgs = [img_load(n) for n in tqdm(test_png)]\n",
    "\n",
    "train_imgs = np.array(train_imgs)\n",
    "test_imgs = np.array(test_imgs)\n",
    "\n",
    "train_y = pd.read_csv(\"raw_data/train_df.csv\")\n",
    "\n",
    "train_labels = train_y[\"label\"]\n",
    "label_unique = sorted(np.unique(train_labels))\n",
    "label_unique = {key : value for key, value in zip(label_unique, range(len(label_unique)))}\n",
    "\n",
    "train_imgs.shape, train_labels.shape, test_imgs.shape, train_imgs.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = train_imgs.astype(\"float32\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(train_imgs, train_labels,\n",
    "                                                  test_size=VALIDATION_SPLIT, random_state=SEED, stratify=train_labels)\n",
    "\n",
    "y_train = [label_unique[k] for k in y_train]\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "y_val = [label_unique[k] for k in y_val]\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "y_train=tf.keras.utils.to_categorical(y_train)\n",
    "y_val=tf.keras.utils.to_categorical(y_val)\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = tf.keras.Sequential([\n",
    "        layers.experimental.preprocessing.RandomFlip(\"vertical\"),\n",
    "        layers.experimental.preprocessing.RandomCrop(int(resize_size*0.9), int(resize_size*0.9)),\n",
    "        layers.experimental.preprocessing.Resizing(resize_size, resize_size),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.5, fill_mode='constant', fill_value=0.0),\n",
    "])\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    .shuffle(len(X_train))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(lambda x, y: (augmentation(x, training=True), y),\n",
    "         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights=\"imagenet\",\n",
    "    pooling='avg',\n",
    ")\n",
    "\n",
    "model=tf.keras.Sequential([\n",
    "    pretrained_model,\n",
    "    tf.keras.layers.Dense(y_train.shape[1], activation=\"softmax\")\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optim,\n",
    "    loss=loss_function,\n",
    "    metrics=tfa.metrics.F1Score(num_classes=y_train.shape[1], average=\"macro\")\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path=f\"load_model/{parser.description}\"\n",
    "\n",
    "callback = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_f1_score',\n",
    "        patience=5,\n",
    "        mode=\"max\",\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor=\"val_f1_score\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "]\n",
    "\n",
    "if args.class_weight==True:\n",
    "    class_weight = np.argmax(y_train, axis=1)\n",
    "    class_weight = (1 / np.bincount(class_weight)) * (len(class_weight))\n",
    "    class_weight = class_weight / class_weight.sum()\n",
    "    class_weight = {key : value for key, value in zip(range(len(class_weight)), class_weight)}\n",
    "else:\n",
    "    class_weight = None\n",
    "\n",
    "history=model.fit(\n",
    "    train_ds,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[callback, WandbCallback()],\n",
    "    validation_data=val_ds,\n",
    "    class_weight=class_weight\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['f1_score']\n",
    "val_acc = history.history['val_f1_score']\n",
    "\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "plt.plot(acc, label='Training Macro-F1')\n",
    "plt.plot(val_acc, label='Validation Macro-F1')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Macro-F1')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((test_imgs))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mcYSOn-m1d54"
   },
   "outputs": [],
   "source": [
    "pred_prob = model.predict(test_ds)\n",
    "f_pred = np.argmax(pred_prob, axis=1)\n",
    "label_decoder = {val:key for key, val in label_unique.items()}\n",
    "f_result = [label_decoder[result] for result in f_pred]\n",
    "\n",
    "pd.Series(f_result).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bfcgt_fI_AWw"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"raw_data/sample_submission.csv\")\n",
    "submission[\"label\"] = f_result\n",
    "submission.to_csv(f\"{parser.description}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPxWDbXdBZDPv2XiwW8C1k0",
   "collapsed_sections": [],
   "mount_file_id": "13o4BpF8zzuXcEiNVlVG2KwTXMW1Y55_v",
   "name": "test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
