{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SynAug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 112690,
     "status": "ok",
     "timestamp": 1645972270802,
     "user": {
      "displayName": "‍김태형[ 대학원석·박사통합과정재학 / 산업경영공학과 ]",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GjFNpnkjN-Em0rOki5hhy0HR7yGAbxSpzCjHV0A=s64",
      "userId": "00288066936238655028"
     },
     "user_tz": -540
    },
    "id": "A1IbqGhzB7fy",
    "outputId": "fd656f36-afcb-4871-ee1c-557fd033af76"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import cv2\n",
    "import gc\n",
    "\n",
    "import argparse\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "wandb.init(project=\"DACON_235894\", name=\"SynAug\")\n",
    "\n",
    "parser = argparse.ArgumentParser(description='SynAug')\n",
    "parser.add_argument('--resize_size', default=224, type=int)\n",
    "parser.add_argument('--optimizer', default=\"adam\", type=str) # adam or sgd\n",
    "parser.add_argument('--learning_rate', default=0.0003, type=float)\n",
    "parser.add_argument('--label_smoothing', default=0, type=float) # 0 or 0.1\n",
    "parser.add_argument('--batch_size', default=32, type=int)\n",
    "parser.add_argument('--epochs', default=100, type=int)\n",
    "parser.add_argument('--validation_split', default=0.2, type=float)\n",
    "parser.add_argument('--seed', default=1011, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "wandb.config.update(args)\n",
    "\n",
    "resize_size=args.resize_size\n",
    "BATCH_SIZE=args.batch_size\n",
    "EPOCHS=args.epochs\n",
    "VALIDATION_SPLIT=args.validation_split\n",
    "SEED=args.seed\n",
    "\n",
    "if args.optimizer == \"adam\":\n",
    "    lr = tf.keras.optimizers.schedules.CosineDecay(args.learning_rate, decay_steps=1000)\n",
    "    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "elif args.optimizer == \"sgd\":\n",
    "    optim = f.keras.optimizers.SGD(learning_rate=args.learning_rate, momentum=0.9)\n",
    "\n",
    "loss_function = tf.keras.losses.CategoricalCrossentropy(label_smoothing=args.label_smoothing)\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "set_seeds()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_load(path):\n",
    "    img = cv2.imread(path)[:,:,::-1]\n",
    "    img = tf.image.central_crop(img, 0.9).numpy()\n",
    "    img = cv2.resize(img, (resize_size, resize_size), cv2.INTER_AREA)\n",
    "    return img\n",
    "\n",
    "train_png = sorted(glob('raw_data/train/*.png'))\n",
    "test_png = sorted(glob('raw_data/test/*.png'))\n",
    "\n",
    "train_imgs = [img_load(m) for m in tqdm(train_png)]\n",
    "test_imgs = [img_load(n) for n in tqdm(test_png)]\n",
    "\n",
    "train_imgs = np.array(train_imgs)\n",
    "test_imgs = np.array(test_imgs)\n",
    "\n",
    "train_y = pd.read_csv(\"raw_data/train_df.csv\")\n",
    "\n",
    "train_labels = train_y[\"label\"]\n",
    "label_unique = sorted(np.unique(train_labels))\n",
    "label_unique = {key : value for key, value in zip(label_unique, range(len(label_unique)))}\n",
    "\n",
    "train_imgs.shape, train_labels.shape, test_imgs.shape, train_imgs.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation : synthetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_load_gray(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    img = cv2.resize(img, (resize_size, resize_size), cv2.INTER_AREA)\n",
    "    return img\n",
    "\n",
    "gt_png = sorted(glob('gt_data/*.png'))\n",
    "gt_imgs = [img_load_gray(n) for n in tqdm(gt_png)]\n",
    "gt_imgs = np.array(gt_imgs)[:, :, :, np.newaxis]\n",
    "\n",
    "gt_y = train_y.set_index(\"file_name\").loc[os.listdir('gt_data/')].reset_index()\n",
    "\n",
    "mask_imgs = np.where(gt_imgs!=0, True, False)\n",
    "unmask_imgs = np.where(gt_imgs==0, True, False)\n",
    "\n",
    "bad_imgs = train_imgs[gt_y[\"index\"]]\n",
    "bad_imgs *= mask_imgs\n",
    "\n",
    "good_imgs = train_imgs[train_y[\"state\"]==\"good\"]\n",
    "good_y = train_y[train_y[\"state\"]==\"good\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_imgs=[]\n",
    "syn_labels=[]\n",
    "\n",
    "for c in tqdm(train_y[\"class\"].unique()):\n",
    "    \n",
    "    if c == \"cable\":\n",
    "        \n",
    "        temp_good_y = good_y[good_y[\"class\"]==c]\n",
    "        temp_bad_y = gt_y[(gt_y['class'] == c) & (gt_y['label'] != 'cable-cable_swap')]\n",
    "\n",
    "        good_len = len(temp_good_y)\n",
    "        bad_len = len(temp_bad_y)\n",
    "\n",
    "        temp_good_imgs = train_imgs[temp_good_y[\"index\"]]\n",
    "        temp_unmask_imgs = unmask_imgs[(gt_y['class'] == c) & (gt_y['label'] != 'cable-cable_swap')]\n",
    "        temp_bad_imgs = bad_imgs[(gt_y['class'] == c) & (gt_y['label'] != 'cable-cable_swap')]\n",
    "\n",
    "        temp_good_imgs = np.repeat(temp_good_imgs, bad_len, axis=0)\n",
    "        temp_unmask_imgs = np.tile(temp_unmask_imgs, (good_len, 1, 1, 1))\n",
    "        temp_bad_imgs = np.tile(temp_bad_imgs, (good_len, 1, 1, 1))\n",
    "        temp_bad_labels = np.tile(temp_bad_y[\"label\"].values, good_len)\n",
    "\n",
    "        temp_good_imgs *= temp_unmask_imgs\n",
    "\n",
    "        syn_imgs.append(temp_good_imgs + temp_bad_imgs)\n",
    "        syn_labels.append(temp_bad_labels)\n",
    "    \n",
    "    elif c == \"metal_nut\":\n",
    "        \n",
    "        temp_good_y = good_y[good_y[\"class\"]==c]\n",
    "        temp_bad_y = gt_y[(gt_y['class'] == c) & (gt_y['label'] != 'metal_nut-flip')]\n",
    "\n",
    "        good_len = len(temp_good_y)\n",
    "        bad_len = len(temp_bad_y)\n",
    "\n",
    "        temp_good_imgs = train_imgs[temp_good_y[\"index\"]]\n",
    "        temp_unmask_imgs = unmask_imgs[(gt_y['class'] == c) & (gt_y['label'] != 'metal_nut-flip')]\n",
    "        temp_bad_imgs = bad_imgs[(gt_y['class'] == c) & (gt_y['label'] != 'metal_nut-flip')]\n",
    "\n",
    "        temp_good_imgs = np.repeat(temp_good_imgs, bad_len, axis=0)\n",
    "        temp_unmask_imgs = np.tile(temp_unmask_imgs, (good_len, 1, 1, 1))\n",
    "        temp_bad_imgs = np.tile(temp_bad_imgs, (good_len, 1, 1, 1))\n",
    "        temp_bad_labels = np.tile(temp_bad_y[\"label\"].values, good_len)\n",
    "\n",
    "        temp_good_imgs *= temp_unmask_imgs\n",
    "\n",
    "        syn_imgs.append(temp_good_imgs + temp_bad_imgs)\n",
    "        syn_labels.append(temp_bad_labels)\n",
    "        \n",
    "    elif c != \"screw\":\n",
    "        \n",
    "        temp_good_y = good_y[good_y[\"class\"]==c]\n",
    "        temp_bad_y = gt_y[gt_y[\"class\"]==c]\n",
    "\n",
    "        good_len = len(temp_good_y)\n",
    "        bad_len = len(temp_bad_y)\n",
    "\n",
    "        temp_good_imgs = train_imgs[temp_good_y[\"index\"]]\n",
    "        temp_unmask_imgs = unmask_imgs[gt_y[\"class\"]==c]\n",
    "        temp_bad_imgs = bad_imgs[gt_y[\"class\"]==c]\n",
    "\n",
    "        temp_good_imgs = np.repeat(temp_good_imgs, bad_len, axis=0)\n",
    "        temp_unmask_imgs = np.tile(temp_unmask_imgs, (good_len, 1, 1, 1))\n",
    "        temp_bad_imgs = np.tile(temp_bad_imgs, (good_len, 1, 1, 1))\n",
    "        temp_bad_labels = np.tile(temp_bad_y[\"label\"].values, good_len)\n",
    "\n",
    "        temp_good_imgs *= temp_unmask_imgs\n",
    "\n",
    "        syn_imgs.append(temp_good_imgs + temp_bad_imgs)\n",
    "        syn_labels.append(temp_bad_labels)\n",
    "\n",
    "syn_imgs = np.vstack(syn_imgs)\n",
    "syn_labels = np.hstack(syn_labels)\n",
    "\n",
    "syn_imgs.shape, syn_labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation : rotate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aug_rotate(imgs, labels, times):\n",
    "    aug_imgs = []\n",
    "    for i in range(times):\n",
    "        if i==0:\n",
    "            aug_imgs.append(tfa.image.rotate(imgs, tf.constant(np.pi),\n",
    "                                             fill_mode=\"nearest\"))\n",
    "        else:\n",
    "            aug_imgs.append(tfa.image.rotate(imgs, tf.constant(np.pi/i),\n",
    "                                             fill_mode=\"nearest\"))\n",
    "    aug_labels = np.tile(labels, times)\n",
    "    return np.vstack(aug_imgs), aug_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cable_imgs = train_imgs[train_y[\"label\"]=='cable-cable_swap']\n",
    "cable_labels = train_labels[train_y[\"label\"]=='cable-cable_swap']\n",
    "\n",
    "cable_imgs, cable_labels = aug_rotate(cable_imgs, cable_labels, 180)\n",
    "\n",
    "cable_imgs.shape, cable_labels.shape, cable_imgs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metal_imgs = train_imgs[train_y[\"label\"]=='metal_nut-flip']\n",
    "metal_labels = train_labels[train_y[\"label\"]=='metal_nut-flip']\n",
    "\n",
    "metal_imgs, metal_labels = aug_rotate(metal_imgs, metal_labels, 90)\n",
    "\n",
    "metal_imgs.shape, metal_labels.shape, metal_imgs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screw_imgs = train_imgs[(train_y['state'] != \"good\") & (train_y['class'] == 'screw')]\n",
    "screw_labels = train_labels[(train_y['state'] != \"good\") & (train_y['class'] == 'screw')]\n",
    "\n",
    "screw_imgs, screw_labels = aug_rotate(screw_imgs, screw_labels, 90)\n",
    "\n",
    "screw_imgs.shape, screw_labels.shape, screw_imgs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tooth_imgs = train_imgs[train_y[\"label\"]=='toothbrush-good']\n",
    "tooth_labels = train_labels[train_y[\"label\"]=='toothbrush-good']\n",
    "\n",
    "tooth_imgs, tooth_labels = aug_rotate(tooth_imgs, tooth_labels, 18)\n",
    "\n",
    "tooth_imgs.shape, tooth_labels.shape, tooth_imgs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_imgs = train_imgs[train_y[\"state\"]=='good']\n",
    "good_labels = train_labels[train_y[\"state\"]=='good']\n",
    "\n",
    "good_imgs, good_labels = aug_rotate(good_imgs, good_labels, 4)\n",
    "\n",
    "good_imgs.shape, good_labels.shape, good_imgs.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Augmentation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_imgs = np.concatenate((train_imgs, syn_imgs, good_imgs,\n",
    "                           screw_imgs, metal_imgs, cable_imgs, tooth_imgs), axis=0)\n",
    "aug_labels=np.concatenate((train_labels, syn_labels, good_labels,\n",
    "                           screw_labels, metal_labels, cable_labels, tooth_labels), axis=0)\n",
    "\n",
    "aug_imgs.shape, aug_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_func(data):\n",
    "    N = len(data)\n",
    "    sample_n = 200\n",
    "    sample = data.take(np.random.permutation(N)[:sample_n])\n",
    "    return sample\n",
    "\n",
    "sample_y=pd.DataFrame(aug_labels, columns=[\"label\"]).groupby('label', group_keys=False).apply(sampling_func)\n",
    "\n",
    "sample_imgs = aug_imgs[sample_y.index]\n",
    "sample_labels = aug_labels[sample_y.index]\n",
    "\n",
    "sample_imgs.shape, sample_labels.shape, sample_imgs.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_imgs, train_y, train_labels, gt_imgs, gt_y,\n",
    "del mask_imgs, unmask_imgs, bad_imgs, good_imgs, good_y, good_labels\n",
    "del temp_good_imgs, temp_good_y, temp_unmask_imgs\n",
    "del temp_bad_imgs, temp_bad_y, temp_bad_labels\n",
    "del syn_imgs, screw_imgs, metal_imgs, cable_imgs, tooth_imgs\n",
    "del syn_labels, screw_labels, metal_labels, cable_labels, tooth_labels\n",
    "del aug_imgs, aug_labels, sample_y\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_imgs = sample_imgs.astype(\"float32\")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(sample_imgs, sample_labels,\n",
    "                                                  test_size=VALIDATION_SPLIT, random_state=SEED, stratify=sample_labels)\n",
    "\n",
    "y_train = [label_unique[k] for k in y_train]\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "y_val = [label_unique[k] for k in y_val]\n",
    "y_val = np.array(y_val)\n",
    "\n",
    "y_train=tf.keras.utils.to_categorical(y_train)\n",
    "y_val=tf.keras.utils.to_categorical(y_val)\n",
    "\n",
    "X_train.shape, X_val.shape, y_train.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation = tf.keras.Sequential([\n",
    "        layers.experimental.preprocessing.RandomFlip(\"vertical\"),\n",
    "        layers.experimental.preprocessing.RandomCrop(int(resize_size*0.9), int(resize_size*0.9)),\n",
    "        layers.experimental.preprocessing.Resizing(resize_size, resize_size),\n",
    "        layers.experimental.preprocessing.RandomRotation(0.5, fill_mode='constant', fill_value=0.0),\n",
    "])\n",
    "\n",
    "train_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "    .shuffle(len(X_train))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .map(lambda x, y: (augmentation(x, training=True), y),\n",
    "         num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")\n",
    "\n",
    "val_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((X_val, y_val))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_encoder():\n",
    "        \n",
    "    sampling_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights=\"imagenet\",\n",
    "        pooling='avg',\n",
    "    )\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(resize_size, resize_size, 3))\n",
    "    outputs = sampling_model(inputs)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"encoder\")\n",
    "        \n",
    "    return model\n",
    "\n",
    "encoder = create_encoder()\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_classifier(encoder):\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(resize_size, resize_size, 3))\n",
    "    features = encoder(inputs)\n",
    "    outputs = layers.Dense(y_train.shape[1], activation=\"softmax\")(features)\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=outputs, name=\"classifier\")\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optim,\n",
    "        loss=loss_function,\n",
    "        metrics=tfa.metrics.F1Score(num_classes=y_train.shape[1], average=\"macro\")\n",
    "    )\n",
    "        \n",
    "    return model\n",
    "\n",
    "classifier = create_classifier(encoder)\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path=f\"load_model/{parser.description}\"\n",
    "\n",
    "callback = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_f1_score',\n",
    "        patience=5,\n",
    "        mode=\"max\",\n",
    "        restore_best_weights=True,\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor=\"val_f1_score\",\n",
    "        save_best_only=True,\n",
    "        save_weights_only=True,\n",
    "        mode=\"max\",\n",
    "    )\n",
    "]\n",
    "\n",
    "history=classifier.fit(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=[callback, WandbCallback()],\n",
    "    validation_data=val_ds,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history.history['f1_score']\n",
    "val_acc = history.history['val_f1_score']\n",
    "\n",
    "loss=history.history['loss']\n",
    "val_loss=history.history['val_loss']\n",
    "\n",
    "plt.plot(acc, label='Training Macro-F1')\n",
    "plt.plot(val_acc, label='Validation Macro-F1')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Macro-F1')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.load_weights(checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = (\n",
    "    tf.data.Dataset.from_tensor_slices((test_imgs))\n",
    "    .batch(BATCH_SIZE)\n",
    "    .prefetch(tf.data.experimental.AUTOTUNE)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_prob = classifier.predict(test_ds)\n",
    "f_pred = np.argmax(pred_prob, axis=1)\n",
    "label_decoder = {val:key for key, val in label_unique.items()}\n",
    "f_result = [label_decoder[result] for result in f_pred]\n",
    "\n",
    "pd.Series(f_result).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bfcgt_fI_AWw"
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"raw_data/sample_submission.csv\")\n",
    "submission[\"label\"] = f_result\n",
    "submission.to_csv(f\"{parser.description}.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPxWDbXdBZDPv2XiwW8C1k0",
   "collapsed_sections": [],
   "mount_file_id": "13o4BpF8zzuXcEiNVlVG2KwTXMW1Y55_v",
   "name": "test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
